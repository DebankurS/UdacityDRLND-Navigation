{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning - Navigation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to train an agent to achieve an average reward of atleast +13 over 100 consecutive episodes in the provided environment.\n",
    "\n",
    "The environment provided is that of a Unity ML Agents scenario.\n",
    "\n",
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.\n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around the agent's forward direction. Given this information, the agent has to learn how to best select actions. Four discrete actions are available, corresponding to:\n",
    "\n",
    "* 0 - move forward.\n",
    "* 1 - move backward.\n",
    "* 2 - turn left.\n",
    "* 3 - turn right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project was solved using Deep Reinforcement Learning using a Deep Q-Network.\n",
    "\n",
    "The base code of the project is derived from the solution provided in Udacity Deep Learning Nanodegree Github repo for solving the lunar lander scenario from OpenAI Gym.\n",
    "\n",
    "https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn\n",
    "\n",
    "The same solution was modified and updated for the Unity ML Agents environment provided.\n",
    "\n",
    "* The notebook **Navigation.ipynb** contains the implementation for training and visualising the untrained agent initially. Then the training code is implemented and at the end we can visualize the Trained agent working on the provided environment.\n",
    "\n",
    "* **dqn_agent.py** contains the code to understand and determine how the agent interacts with the environment and learns to optimize the reward.\n",
    "\n",
    "* **model.py** contains the architecture of the deep learning model used in this implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deep Q-Learning algorithm(DQN) was chosen to solve the agent-environment interaction.\n",
    "\n",
    "Q-Learning is one form of TD(Temporal Difference) Learning which enables us to update the Q-table at every timestep. In Deep Q-Learning, a neural network is used as a function approximator to map the relationship between the states and actions and returns the respective Q-Values for every state-action pair.\n",
    "\n",
    "### Experience Replay and Fixed Q-Targets\n",
    "Using the general DQN Algorithm the experiences are stored in the form of (State,Action,Values,Next_State) tuples. But this can cause a lot of correlation between the sequences if the agent learns while interacting.\n",
    "\n",
    "To get rid of this issue, we separate the training and interaction loops into two different networks called the target and local network. This is using Fixed Q-Targets.\n",
    "\n",
    "The local network interacts with the environment and stores its experiences in a buffer.\n",
    "At every predefined set of intervals, the target network gets a random sample of predefined size from the buffer and learns from it. This implementation is called Experience Replay.\n",
    "\n",
    "The local network can then determine in a Supervised Learning fashion by comparing its outputs with that of the target network and compute the loss, hence \"learning\" better.\n",
    "\n",
    "### Sync the local and target network\n",
    "Once the local network learns form the replay buffer, the model parameters of the target network are updated with that of the local network with respect to the relationship below\n",
    "\n",
    "θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "### Model Architecture\n",
    "The Deep Learning Model used here is a Multilayer Perceptron which returns the Q-values for every state-action value pair. It is defined in the file model.py\n",
    "\n",
    "* The model has 2 fully connected layers with 64 nodes.\n",
    "* It takes in an input equal to the state size provided which in this case is 37\n",
    "* Relu activation function is used betweeen the two layers\n",
    "* Adam optimizer and Mean-Squared Error loss is used by the neural network to \"learn\"\n",
    "\n",
    "### Agent\n",
    "\n",
    "The agent is defined in the agent.py file. It is the Deep Q-Learning agent which interacts with the environment. It references the local and target network from the model defined in model.py.\n",
    "\n",
    "It contains four methods:\n",
    "\n",
    "**Step**: Here the agent saves the experiences in the replay memory. After a certain set of predefined intervals, it also causes the network to learn from the replay buffer.\n",
    "\n",
    "**Act**: Here the agent returns the actions based on the current policy using Epsilon-Greedy Action Selection\n",
    "\n",
    "**Learn**: This is where the agent actually learns. We randomly sample a batch from the experience buffer in the form of (states, actions, rewards, next_states, dones) and pass on the next state to the current target network and from there we compute the current Q-Values for the current states.\n",
    "Then using the same states, we obtain the expected Q-values for the local network.\n",
    "Comparing these two sets of Q-Values we obtain the losses and update the local network parameters in a Supervised Learning fashion.\n",
    "\n",
    "Then we run the soft_update function to update the target network with the local network parameters\n",
    "\n",
    "**Soft_update**: Here we update the target network with the local network parameters using the formula\n",
    "                    \n",
    "                    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "The hyperparameters used to train the agent are:\n",
    "\n",
    "\n",
    "* BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "* BATCH_SIZE = 128         # minibatch size\n",
    "* GAMMA = 0.99            # discount factor\n",
    "* TAU = 1e-3              # for soft update of target parameters\n",
    "* LR = 5e-4               # learning rate \n",
    "* UPDATE_EVERY = 4        # how often to update the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of Rewards\n",
    "\n",
    "The plot of average rewards for every 100 episodes are listed below.\n",
    "\n",
    "* Episode 100\tAverage Score: 0.75\n",
    "* Episode 200\tAverage Score: 3.84\n",
    "* Episode 300\tAverage Score: 7.69\n",
    "* Episode 400\tAverage Score: 9.98\n",
    "* Episode 470\tAverage Score: 13.00\n",
    "\n",
    "Environment solved in 470 episodes!\tAverage Score: 13.00\n",
    "\n",
    "![](Plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work\n",
    "\n",
    "The Reinforcement Learning agent was trained using Deep Q network using Experience Replay and Fixed Q targets\n",
    "\n",
    "* Using the existing methodology, the hyperparameters can definitely be tuned to get a faster response and higher rewards.\n",
    "* Other methodologies can include using [Double DQN](https://arxiv.org/abs/1509.06461), [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952) or using [Duelling DQN](https://arxiv.org/abs/1511.06581)\n",
    "* There are many more modifications to the DQN Algorithm, many of which are incorporated in the [Rainbow DQN](https://arxiv.org/abs/1710.02298) implementation which includes a total of 6 modifications to the traditional DQN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
